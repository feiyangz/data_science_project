---
title: "data_science_project"
output:
  html_document: default
  word_document: default
---
```{r}
#glassdoor
#find the website of data scientist positions in the united states from glassdoor
library(rvest)
library(stringr)
library(dplyr)
library(ggplot2)
library(httr)

Basic.url <- "https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14.htm"

start.page <-read_html(Basic.url) 
# count total pages
page_count <- unlist(strsplit(start.page %>% 
                               html_node(".padVertSm") %>%
                               html_text(), split = ' ')) 
page_count <- as.numeric(str_replace_all(page_count[length(page_count)],',',''))
cat('Total page count: ', page_count)
urls <-paste0("https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14_IP", 1:page_count, ".htm")
#we have 892 pages total

#In this project, I will only collect data from the first 100 pages.
url.1<- paste0("https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14_IP", 1:35, ".htm")
#get each sub urls
subpage<- rep(NA,10000)
 k=0
for (i in 1:length(url.1)){
  url.2=url.1[i]
   fields <- url.2 %>% read_html() %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "jl", " " ))]')
    job.urls.1 <- fields%>% html_nodes("a.jobLink") %>% html_attr("href")
    job.urls.1 <-job.urls.1[c(seq(1,60,by=2))]
    job.urls<- unique(paste('https://www.glassdoor.com', job.urls.1, sep = ''))
    subpage.2=unlist(job.urls,recursive = FALSE)
    subpage[(k+1):(k+length(subpage.2))]=subpage.2
    k=k+30
    Sys.sleep(1)
}
subpage<- ifelse(grepl("NA",subpage)==TRUE,NA,subpage) 
subpage<-subpage[!is.na(subpage)]
l=length(subpage)
company <- rep(NA, l)
location <- rep(NA, l)
industry <- rep(NA, l)
Python<-rep(NA, l)
R<-rep(NA, l)
SAS<-rep(NA, l)
SQL<-rep(NA, l)
Java<-rep(NA,l)
Tableau<-rep(NA, l)
Spark<-rep(NA, l)
C<-rep(NA, l)
Perl<-rep(NA, l)
Excel<-rep(NA, l)


#lapply(url.1, function(url.1) {
  #fields <- url.1 %>% read_html() %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "jl", " " ))]')
 #job.urls <- fields%>% html_nodes("a.jobLink") %>% html_attr("href")
 #job.urls <-job.urls[c(seq(1,60,by=2))]
#job.urls = unique(paste('https://www.glassdoor.com', job.urls, sep = ''))
#}
#)
#subpage=unlist(subpage.2,recursive = FALSE)
#subpage<- ifelse(grepl("NA",subpage)==TRUE,NA,subpage)








#collect company name, location sector and techniques(Python,R,SAS,SQL,Java,Tableau,Spark,C++,Perl,Excel)
for(j in 1:length(subpage)){
   subpage.1=subpage[j]
   job <- html_session(subpage.1)
  tem<- tryCatch({job %>%read_html()%>% html_text())
   company[j] <- job %>%read_html() %>% html_node(".padRtSm") %>% html_text()
      location[j]<- job %>% html_node(".subtle") %>% html_text()
 
desc <- job %>% read_html() %>% html_nodes("#JobDescContainer") %>% html_text()
  Python[j] <- any(grepl("python", desc, ignore.case=TRUE))
  R[j] <- any(grepl("\\bR\\b", desc, ignore.case=TRUE))
  SAS[j] <- any(grepl("\\bSAS\\b", desc, ignore.case=TRUE))
  SQL[j]<- any(grepl("\\bSQL\\b", desc, ignore.case=TRUE))
  Java[j] <- any(grepl("\\bJava\\b", desc, ignore.case=TRUE))
  Tableau[j] <- any(grepl("\\bTableau\\b", desc, ignore.case=TRUE))
  Spark[j] <- any(grepl("\\bSpark\\b", desc, ignore.case=TRUE))
   C[j]<- any(grepl("\\bC\\b", desc, ignore.case=TRUE))
   Perl[j] <- any(grepl("\\bPerl\\b", desc, ignore.case=TRUE))
   Excel[j] <- any(grepl("\\bExcel\\b", desc, ignore.case=TRUE))
   Sys.sleep(1)
   }      

location<-str_sub(location,-2,-1)

data <- data.frame("Company" = company, "Location"=location, "Industry"=industry, "Python"=Python, "R"=R,"SAS"=SAS,"SQL"=SQL,"Java"=Java,"Tableau"=Tableau,"Spark"=Spark,"C"=C,"Perl"=Perl,"Excel"=Excel)
data <-data[c(1:length(subpage)),]
data$Website<-subpage

```






```{r}
#find the website of data scientist positions in the united states from the data jobs

Basic.url <- "https://datajobs.com/Data-Science-Jobs"
url.1 <-paste0("https://datajobs.com/Data-Science-Jobs","~",1:50)
start.page <-read_html(Basic.url) 
#count pages we have (by search for "next page")
res.2 <- sapply(url.1, function(x) {
  desc.2 <- x %>% read_html() %>% html_nodes("div:nth-child(26)") %>% html_text()
  NEXT <- any(grepl("\\NEXT\\b", desc.2, ignore.case=TRUE))
})
Page.num <- length(which(res.2==TRUE))+1
url <-paste0("https://datajobs.com/Data-Science-Jobs","~",1:Page.num)


#company
company <- lapply(url,
                  function(url){
                    url %>% read_html() %>% 
                      html_nodes("a .stealth-header") %>% 
                      html_text()
                  })
company1=unlist(company, recursive = FALSE)

#location
location <- lapply(url,
                   function(url){
                     url %>% read_html() %>% 
                       html_nodes("em .stealth-header") %>% 
                       html_text()
                   })
location1=unlist(location, recursive = FALSE)

#single job URL for all the pages
page.links <- lapply(url,
                     function(url){
                       url %>%read_html() %>%
                         html_nodes(xpath = '//div[contains(concat( " ", @class, " " ), concat( " ", "stealth-header", " " ))]//a') %>%
                         html_attr('href')
                     })
page.links1 <- unlist(page.links, recursive = FALSE)
job.urls <-lapply(page.links1, function(x) paste0("https://datajobs.com/", x))
job.urls1 <- unlist(job.urls, recursive = FALSE)

#skills(python,R,SAS,SQL,Java,Tableau,Spark,C++,Perl,Excel)
res <- sapply(job.urls1, function(x) {
  desc <- x %>% read_html() %>% html_nodes("#job_description .jobpost-table-cell-2 , #job_description .jobpost-table-cell-2 strong") %>% html_text()
  Python <- any(grepl("python", desc, ignore.case=TRUE))
  R <- any(grepl("\\bR\\b", desc, ignore.case=TRUE))
  SAS <- any(grepl("\\bSAS\\b", desc, ignore.case=TRUE))
  SQL <- any(grepl("\\bSQL\\b", desc, ignore.case=TRUE))
  Java <- any(grepl("\\bJava\\b", desc, ignore.case=TRUE))
  Tableau <- any(grepl("\\bTableau\\b", desc, ignore.case=TRUE))
  Spark <- any(grepl("\\bSpark\\b", desc, ignore.case=TRUE))
  C<- any(grepl("\\bC\\b", desc, ignore.case=TRUE))
  Perl <- any(grepl("\\bPerl\\b", desc, ignore.case=TRUE))
  Excel <- any(grepl("\\bExcel\\b", desc, ignore.case=TRUE))
  
  # info <- paste0(x %>% read_html() %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "-key", " " ))]') %>% html_text(),
  # x %>% read_html() %>% html_nodes(xpath='//*[contains(concat( " ", #@class, " " ), concat( " ", "-value", " " ))]') %>% html_text())
  #  ind <- grep("Industry", info)
  # sector <- ifelse(length(ind) == 0, NA, gsub("Industry: ", "", info[grep("Industry", info)]))
  
  c(Python=Python, R=R,SAS=SAS,SQL=SQL,Java=Java,Tableau=Tableau,Spark=Spark,C=C,Perl=Perl,Excel=Excel)
})
res <- unname(res)


data <- data.frame("Company" = company1, "Location"=location1, "Python"=res[1,], "R"=res[2,], "SAS"=res[3,],"SQL"=res[4,],"Java"=res[5,],"Tableau"=res[6,],"Spark"=res[7,],"C"=res[8,],"Perl"=res[9,],"Excel"=res[10,],"Website"=job.urls1)


```
```








