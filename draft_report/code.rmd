---
title: "data_science_project"
output:
  html_document: default
  word_document: default
---
## web scraping
##collected positions from 
```{r,echo=FALSE}
#glassdoor
#find the website of data scientist positions in the united states from glassdoor
library(rvest)
library(stringr)



Basic.url <- "https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14.htm"

start.page <-read_html(Basic.url) 
# count total pages
page_count <- unlist(strsplit(start.page %>% 
                               html_node(".padVertSm") %>%
                               html_text(), split = ' ')) 
page_count <- as.numeric(str_replace_all(page_count[length(page_count)],',',''))
cat('Total page count: ', page_count)
urls.1<-paste0("https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14_IP", 1:page_count, ".htm")
#we have 892 pages total

#In this project, I will only collect data from the first 33 pages.
url.1<- paste0("https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14_IP", 1:33, ".htm")



#get each sub urls
subpage<- rep(NA,10000)
rating<- rep(NA,10000)
salaries<- rep(NA,10000)


k=0
for (i in 1:length(url.1)){
  url.2=url.1[i]
   fields <- url.2 %>% read_html() %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "jl", " " ))]')
    job.urls.1 <- fields%>% html_nodes("a.jobLink") %>% html_attr("href")
    job.urls.1 <-job.urls.1[c(seq(1,60,by=2))]
    job.urls<- unique(paste('https://www.glassdoor.com', job.urls.1, sep = ''))
    subpage.2=unlist(job.urls,recursive = FALSE)
    subpage[(k+1):(k+length(subpage.2))]=subpage.2
  
    salaries[(k+1):(k+length(subpage.2))] <- sapply(fields, function(x) {
  salaries.1 <- html_nodes(x, ".small") %>% html_text()
  ifelse(length(salaries.1) == 0, "NA", trimws(salaries.1))
})
rating[(k+1):(k+length(subpage.2))] <- sapply(fields, function(x) {
  rating.1 <- html_nodes(x, ".compactStars") %>% html_text()
  ifelse(length(rating.1) == 0, "NA", trimws(rating.1))
})    
   k=k+30
Sys.sleep(1)
}    
subpage<- ifelse(grepl("NA",subpage)==TRUE,NA,subpage) 
subpage<-subpage[!is.na(subpage)]
Salaries <-salaries[!is.na(salaries)]
Rating <-rating[!is.na(rating)]
l=length(subpage)
company <- rep(NA, l)
location <- rep(NA, l)
industry <- rep(NA, l)
Python<-rep(NA, l)
R<-rep(NA, l)
SAS<-rep(NA, l)
SQL<-rep(NA, l)
Java<-rep(NA,l)
Tableau<-rep(NA, l)
Spark<-rep(NA, l)
C<-rep(NA, l)
Perl<-rep(NA, l)
Excel<-rep(NA, l)
Hadoop<-rep(NA, l)
NoSQL<-rep(NA, l)
HBase<-rep(NA, l)
  c(company[j],location[j])=c("NA","NA")
  
  
#collect company name, location sector and techniques(Python,R,SAS,SQL,Java,Tableau,Spark,C++,Perl,Excel,Hadoop,NoSQL,HBase)
for(j in 1:length(subpage)){
  subpage.1= subpage[j]
  r<-GET(subpage.1,user_agent("myua"))
   if(status_code(r)>300){ 
     company[j]=NA
     location[j]=NA 
     Python[j]=NA 
     R[j]=NA 
     SAS[j]=NA 
     SQL[j]=NA 
     Java[j]=NA 
     Tableau[j]=NA 
     Spark[j]=NA 
     C[j]=NA 
     Perl[j]=NA 
     Excel[j]=NA
   }
   else{
  job <-subpage.1 %>%html_session()
   tryCatch({job%>%read_html()%>%html_text()},error=function(e){
      company[j]=NA
     location[j]=NA })
  company[j] <- job %>%read_html() %>% html_node(".padRtSm") %>% html_text()
      location[j]<- job %>% html_node(".subtle") %>% html_text()

      tryCatch({readLines(subpage.1)},warning=function(e){
         industry[j]=NA},error=function(w){
  industry[j]=NA})
  desc.1 <- readLines(subpage.1,warn=FALSE)
    industry.1<- desc.1[str_detect(desc.1, "\'sector\'")] %>%
      str_extract("\"(.*)\"") %>% str_sub(2, -2)
    industry.1=unlist(industry.1,recursive = FALSE) 
  industry.1=ifelse(length(industry.1)==0,NA,industry.1) 
    industry[j]=industry.1

 tryCatch({job%>%read_html()%>%html_text()},error=function(f){
  c(Python[j],R[j],SAS[j],SQL[j],Java[j],Tableau[j],Spark[j],C[j],Perl[j],Excel[j],Hadoop[j],NoSQL[j], HBase[j])=rep(NA,13)})
desc <- job %>% read_html() %>% html_nodes("#JobDescContainer") %>% html_text()
  Python[j] <- any(grepl("python", desc, ignore.case=TRUE))
  R[j] <- any(grepl("\\bR\\b", desc, ignore.case=TRUE))
  SAS[j] <- any(grepl("\\bSAS\\b", desc, ignore.case=TRUE))
  SQL[j]<- any(grepl("\\bSQL\\b", desc, ignore.case=TRUE))
  Java[j] <- any(grepl("\\bJava\\b", desc, ignore.case=TRUE))
  Tableau[j] <- any(grepl("\\bTableau\\b", desc, ignore.case=TRUE))
  Spark[j] <- any(grepl("\\bSpark\\b", desc, ignore.case=TRUE))
   C[j]<- any(grepl("\\bC\\b", desc, ignore.case=TRUE))
   Perl[j] <- any(grepl("\\bPerl\\b", desc, ignore.case=TRUE))
   Excel[j] <- any(grepl("\\bExcel\\b", desc, ignore.case=TRUE))
   Hadoop[j]<- any(grepl("\\bHadoop\\b", desc, ignore.case=TRUE))
   NoSQL[j]<- any(grepl("\\bNoSQL\\b", desc, ignore.case=TRUE))
   HBase[j]<- any(grepl("\\bHBase\\b", desc, ignore.case=TRUE))
   Sys.sleep(1)
   }      
}
data <- data.frame("Company" = company, "Location"=location, "Industry"=industry,"Rating"=Rating,"Salaries"=Salaries,"Python"=Python, "R"=R,"SAS"=SAS,"SQL"=SQL,"Java"=Java,"Tableau"=Tableau,"Spark"=Spark,"C"=C,"Perl"=Perl,"Excel"=Excel,"Hadoop"=Hadoop,"NoSQL"=NoSQL,"HBase"=HBase)
data <-data[c(1:length(subpage)),]
data$Website<-subpage
data.1 <- data[,-19]
data.2 <- subset(data.1, !duplicated(data.1))
data.2$State<-str_sub(data.2$Location,-2,-1)
write.csv(data.2,file = "glass_door.csv")
```

##data cleaning
```{r}
library(dplyr)
library(ggplot2)
library(httr)
library("curl")
library("dplyr")
library(hydroTSM)
library(stringr)

data.glassdoor<- read.csv("glass_door.csv")
Companyname_m<-length(which(is.na(data.glassdoor$Company)))
#102 positions miss companies' name

#remove "-" of location
data.glassdoor$Location<-gsub("â€“", "", as.character(data.glassdoor$Location))
#remove leading space of location
data.glassdoor$Location<-str_trim(data.glassdoor$Location,"left")
location_m.1<-length(which(is.na(data.glassdoor$Location)))
location_m.2<-length(which(data.glassdoor$Location=="Remote"))
location_m.3<-length(which(data.glassdoor$Location=="United States"))
#21 position miss location including "remote"and "United States"

indus_m<-length(which(is.na(data.glassdoor$Industry)))
#79 position miss Industry
data.glassdoor$Industry=gsub("&amp", "", as.character(data.glassdoor$Industry))

rating_m<-length(which(is.na(data.glassdoor$Rating)))
##41 position miss rating

skill_m<-length(which(is.na(data.glassdoor[,7:19])))
##13 position miss rating

#create a column named max salary
 data.glassdoor$max=rep(NA,nrow(data.glassdoor))
  data.glassdoor$min=rep(NA,nrow(data.glassdoor))

  for (f in 1:nrow(data.glassdoor)){
   s=data.glassdoor$Salaries[f]
    if (is.na(s) ){
    data.glassdoor$max[f]=NA
    data.glassdoor$min[f]=NA
  }
  else
   max1=unlist(strsplit(as.character(s),'-'))[2]
   max2=unlist(strsplit(as.character(max1),'k'))[1]
 data.glassdoor$max[f]=rm1stchar(max2, n = 1)[1]
     
  min1=unlist(strsplit(as.character(s),'-'))[1]
  min2=unlist(strsplit(as.character(min1),'k'))[1]
 data.glassdoor$min[f]=rm1stchar(min2, n = 1)[1]
  }
 
salary_m<-length(which(is.na(data.glassdoor[,21:22])))
##78 position miss salary 

 #remove missing values(industry,salary,rate,location)
data.final<-data.glassdoor[(complete.cases(data.glassdoor[3:22])),]   
data.final$mean<-(as.numeric(data.final$max)+as.numeric(data.final$min))/2
data.final<-data.final[-which(data.final$Location=="Remote"|data.final$Location=="United States"),]
data.final<-data.final[,-1]
#Therefore, we have 639 positions totally
write.csv(data.final,file = "data.final.csv")

```


```{r,echo=FALSE}
#Plot the distribution of positions in the US
data.final<- read.csv("data.final.csv")

library(ggmap)
library(ggplot2)
library(stringr)
data.location<-as.data.frame(data.final$Location[!is.na(data.final$Location)==TRUE])
colnames(data.location)<-"Location"
data.location$lon<-rep(NA,nrow(data.location))
data.location$lat<-rep(NA,nrow(data.location))

for(l in 1:nrow(data.location)){
  ll=as.character(data.location$Location[l])
b=any(grepl("\\,", ll, ignore.case=TRUE))
      if(length(b)=="FALSE")
{data.location$lon=NA
data.location$lat=NA
}

else
data.location$lon[l]=geocode(ll)[1]
data.location$lat[l]=geocode(ll)[2]
Sys.sleep(1)
}

data.location$lon<-data.location$lon[!is.na(data.location$lon)==TRUE]
data.location<-data.location[!is.na(data.location$lat)==TRUE]
save(data.location,file ="coordinate.rdata")
```

```{r}
library(ggmap)
library(ggplot2)
library(stringr)
load("~/Desktop/jhuc/data science/project/data_science_project1/coordinate.rdata")
long=unlist(data.location$lon)
lat=unlist(data.location$lat)
data.location$lon=long
data.location$lat=lat

data.location=data.location[(complete.cases(data.location)),]   

all_states=map_data("state")

gmap<- ggplot() + geom_polygon(data=all_states, aes(x=long, y=lat, group = group),colour="gray", fill = "gray10")+ geom_point(aes(x=lon, y=lat), data=data.location, alpha=0.2,col="orange",size = 2.5)+ ggtitle("Map of Data Scientist Positions in the US")+scale_x_continuous(limits = c(-140, -50))+labs(y="latitude",x="longitude")

ggsave("gmap.png",width = 10, height = 6)



map<-get_map(location = 'US', zoom = 10)
USAMap = ggmap(map, extent="normal")

```


#skills ranking for the whole dataset
```{r}
library(knitr)

a=1
skill.whole<-rep(NA,13)
for(i in 6:18){
  skill.whole[a]<-length(which(data.final[,i]==TRUE))
a=a+1
  }
skills<-c("Python","R","SAS","SQL","Java","Tableau","Spark","C","Perl","Excel","Hadoop","NoSQL","HBase")
count.whole<-data.frame(unlist(skill.whole))
count.whole$skills<-skills
colnames(count.whole)<-c("Count","Skills")
count.whole$Count<-as.numeric(count.whole$Count)

count.whole$Skills<-factor(count.whole$Skills, levels = count.whole$Skills[order(count.whole$Count)])
count.whole<-count.whole[,c(2,1)]


p_skills<- ggplot(data=count.whole,aes(x=Skills,y=Count),width = 2, height = 3)+geom_bar(stat="identity")+coord_flip()+labs(y="Count",x="Skills")+ggtitle("Count by Data Science Skills")+theme(plot.title = element_text(size=12,hjust =0))

ggsave("p_skills.png",width = 10, height = 4)


kable(count.whole)
```


Skills ranking for different industry. I also add max and min salaries and rating for each industry
limition:location can affect salaries 
```{r}
library(dplyr)
library(ggplot2)
library(gridExtra)
library(ggpubr)
#whole dataset
industryrank<-data.frame(table(data.final$Industry))
industryrank.1<- industryrank[order(industryrank$Freq,decreasing=TRUE),][1:10,]
colnames(industryrank.1)=c("Industry","Count")
industryrank.1$Industry<-  factor(industryrank.1$Industry, levels = industryrank.1$Industry[order(industryrank.1$Count)])
industryrank.1$freq<-round(((industryrank.1$Count)/nrow(data.final))*100,3)

#Four regions without counting AK and HI
#west(11)

West <- data.final %>% filter(grepl("WA|OR|CA|MT|ID|WY|NV|AZ|UT|CO|NM", State, ignore.case = TRUE))
westrank<-data.frame(table(West$Industry))
westrank.1<- westrank[order(westrank$Freq,decreasing=TRUE),][1:5,]
colnames(westrank.1)=c("Industry","Count")
westrank.1$Industry<-  factor(westrank.1$Industry, levels = westrank.1$Industry[order(westrank.1$Count)])


#Midwest(12)
Midwest <- data.final %>% filter(grepl("ND|SD|NE|KS|MN|IA|MO|WI|IL|IN|MI|OH", State, ignore.case = TRUE))
Midwestrank<-data.frame(table(Midwest$Industry))
Midwestrank.1<- Midwestrank[order(Midwestrank$Freq,decreasing=TRUE),][1:5,]
colnames(Midwestrank.1)=c("Industry","Count")
Midwestrank.1$Industry<-  factor(Midwestrank.1$Industry, levels = Midwestrank.1$Industry[order(Midwestrank.1$Count)])


#South(18)
South <- data.final %>% filter(grepl("TX|OK|AR|LA|MS|AL|TN|KY|GA|FL|SC|NV|VA|WV|DC|MD|DE|NC", State, ignore.case = TRUE))

Southrank<-data.frame(table(South$Industry))
Southrank.1<- Southrank[order(Southrank$Freq,decreasing=TRUE),][1:5,]
colnames(Southrank.1)=c("Industry","Count")
Southrank.1$Industry<-  factor(Southrank.1$Industry, levels = Southrank.1$Industry[order(Southrank.1$Count)])

#Northeast(9)
Northeast <- data.final %>% filter(grepl("PA|NJ|NY|CT|MA|VT|NH|ME|RI", State, ignore.case = TRUE))

Northeastrank<-data.frame(table(Northeast$Industry))
Northeastrank.1<- Northeastrank[order(Northeastrank$Freq,decreasing=TRUE),][1:5,]
colnames(Northeastrank.1)=c("Industry","Count")
Northeastrank.1$Industry<-  factor(Northeastrank.1$Industry, levels = Northeastrank.1$Industry[order(Northeastrank.1$Count)])


p1<-ggplot(data=industryrank.1,aes(factor(Industry),y=Count),width = 2, height = 3)+
  geom_bar(stat="identity")+coord_flip()+labs(y="Count",x="Industry",size=3)+ggtitle("US (n=639)")+theme(plot.title = element_text(size=20,hjust = 0))+theme(axis.title.x = element_text(size =20))+theme(axis.text = element_text(size = 20))+theme(axis.title.y = element_text(size =20))

p2<-ggplot(data=westrank.1,aes(factor(Industry),y=Count),width = 2, height = 3)+
geom_bar(stat="identity")+coord_flip()+labs(y="Count",x="Industry",size=3)+ggtitle("West (n=233)")+theme(plot.title = element_text(size=20,hjust = 0))+theme(axis.title.x = element_text(size =20))+theme(axis.text = element_text(size = 20))+theme(axis.title.y = element_text(size =20))

p4<-ggplot(data=Southrank.1,aes(factor(Industry),y=Count),width = 2, height = 3)+
geom_bar(stat="identity")+coord_flip()+labs(y="Count",x="Industry",size=3)+ggtitle("South (n=138)")+theme(plot.title = element_text(size=20,hjust =0))+theme(axis.title.x = element_text(size =20))+theme(axis.text = element_text(size = 20))+theme(axis.title.y = element_text(size =20))

p5<-ggplot(data=Northeastrank.1,aes(factor(Industry),y=Count),width = 1, height = 3)+geom_bar(stat="identity")+coord_flip()+labs(y="Count",x="Industry")+ggtitle("Northeast (n=187)")+theme(plot.title = element_text(size=20,hjust = 0))+theme(axis.title.x = element_text(size =20))+theme(axis.text = element_text(size = 20))+theme(axis.title.y = element_text(size =20))

p3<-ggplot(data=Midwestrank.1,aes(factor(Industry),y=Count))+
geom_bar(stat="identity")+coord_flip()+labs(y="Count",x="Industry")+ggtitle(" Midwest (n=78)")+theme(plot.title = element_text(size=20))+theme(axis.title.x = element_text(size =20))+theme(axis.text = element_text(size = 20))+theme(axis.title.y = element_text(size =20))
p.indus<-ggarrange(p1,p2,p3,p4,p5,ncol = 2,nrow=3)
ggsave("p.indus.png",width = 21, height = 15)                 


```


Linear regression
```{r}
library("MASS")
#classify the states into four regions:West(0), Midwest(1), South(2) and Northeast(3)
#Create a new data frame called data.model
data.model<-data.final
data.model$State<-toupper(data.model$State)

data.model$region=rep(NA,nrow(data.model))
for(i in 1:nrow(data.model)){
  if(grepl("WA|OR|CA|MT|ID|WY|NV|AZ|UT|CO|NM", data.model$State[i])==TRUE){
    data.model$region[i]<-0
  }   
else if(grepl("ND|SD|NE|KS|MN|IA|MO|WI|IL|IN|MI|OH",data.model$State[i])==TRUE){
    data.model$region[i]<-1
  }   

  else if(grepl("TX|OK|AR|LA|MS|AL|TN|KY|GA|FL|SC|NV|VA|WV|DC|MD|DE|NC",data.model$State[i])==TRUE){
    data.model$region[i]<-2
  }   
  else if(grepl("PA|NJ|NY|CT|MA|VT|NH|ME|RI",data.model$State[i])==TRUE){
    data.model$region[i]<-3
  }   
  else
    data.model$region[i]<-NA
  }
#We also classify industry into 4 groups: 0.IT 1. Business Service 2.Finance 3.other industry
data.model$ind<-rep(NA,nrow(data.model))
for (i in 1:nrow(data.model)){
  if(data.model$Industry[i]=="Information Technology"){
    data.model$ind[i]<-0
  }
 else if(data.model$Industry[i]=="Business Services"){
    data.model$ind[i]<-1
  }
   else if(data.model$Industry[i]=="Finance"){
    data.model$ind[i]<-2
   }
  else 
    data.model$ind[i]<-3
}
for (i in 6:18){
  data.model[,6:18]=ifelse(data.model[6:18]==TRUE,1,0)
}

#remove missing values(595 positions)
data.model<-data.model[complete.cases(data.model),]
#fit a linear regression model
fit.1<-lm(mean~factor(ind)+factor(region)+Rating+R+Python+SQL+Hadoop+Spark,data = data.model)
AICS<-stepAIC(fit.1,direction="both")
aicr<-AICS$anova
fit<-lm(mean~factor(ind)+factor(region)+Python+Spark,data = data.model)
summary(fit)
confint(fit)
```

appendix
```{r}
kable(count.whole)
x<-data.frame(unique(data.final$Industry))
colnames(x)<-"Industry Type"
kable(x)
```

