---
title: "final_code"
output: html_document
---
Install packages needed for this project
```{r}

packages<-c("rvest","stringr","dplyr","ggplot2","httr","curl","hydroTSM","ggpubr","gridExtra","MASS","ggmap","tidyr","knitr","reshape2","grid")

for (i in packages){
  if(!require(i,character.only = T,quietly=T,warn.conflicts = F)){
    install.packages(i)
  }
  require(i,character.only = T,quietly=T,warn.conflicts = F)
}
```

#Step 1:  Scraping data from Glassdoor with job title 'data scientist' and " business analyst"
In this process, we collected company’s name, location, industry, rating, and salaries. For skills, we collected Python, R, SAS, SQL, Java, Tableau, Spark, C, Perl, Excel, Hadoop, NoSQL, and HBase. We saved the data set in .csv file "data scientist" position was saved in "glass_door.csv" while "data engineer" was saved in "glass_door_statana.csv". Code is not evaluated.
1.
```{r,eval=FALSE}

#find the website of data scientist positions in the united states from glassdoor
Basic.url <- "https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14.htm"

start.page <-read_html(Basic.url) 

url.1<- paste0("https://www.glassdoor.com/Job/data-scientist-jobs-SRCH_KO0,14_IP", 1:33, ".htm")

#get each sub urls, rating and salaries
subpage<- rep(NA,10000)
rating<- rep(NA,10000)
salaries<- rep(NA,10000)


k=0
for (i in 1:length(url.1)){
  url.2=url.1[i]
   fields <- url.2 %>% read_html() %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "jl", " " ))]')
    job.urls.1 <- fields%>% html_nodes("a.jobLink") %>% html_attr("href")
    job.urls.1 <-job.urls.1[c(seq(1,60,by=2))]
    job.urls<- unique(paste('https://www.glassdoor.com', job.urls.1, sep = ''))
    subpage.2=unlist(job.urls,recursive = FALSE)
    subpage[(k+1):(k+length(subpage.2))]=subpage.2
  
    salaries[(k+1):(k+length(subpage.2))] <- sapply(fields, function(x) {
  salaries.1 <- html_nodes(x, ".small") %>% html_text()
  ifelse(length(salaries.1) == 0, "NA", trimws(salaries.1))
})
rating[(k+1):(k+length(subpage.2))] <- sapply(fields, function(x) {
  rating.1 <- html_nodes(x, ".compactStars") %>% html_text()
  ifelse(length(rating.1) == 0, "NA", trimws(rating.1))
})    
   k=k+30
Sys.sleep(1)
}    
subpage<- ifelse(grepl("NA",subpage)==TRUE,NA,subpage) 
subpage<-subpage[!is.na(subpage)]
Salaries <-salaries[!is.na(salaries)]
Rating <-rating[!is.na(rating)]
l=length(subpage)
company <- rep(NA, l)
location <- rep(NA, l)
industry <- rep(NA, l)
Python<-rep(NA, l)
R<-rep(NA, l)
SAS<-rep(NA, l)
SQL<-rep(NA, l)
Java<-rep(NA,l)
Tableau<-rep(NA, l)
Spark<-rep(NA, l)
C<-rep(NA, l)
Perl<-rep(NA, l)
Excel<-rep(NA, l)
Hadoop<-rep(NA, l)
NoSQL<-rep(NA, l)
HBase<-rep(NA, l)

#collect company’s name, location industry and techniques(Python,R,SAS,SQL,Java,Tableau,Spark,C++,Perl,Excel,Hadoop,NoSQL,HBase)
for(j in 1:length(subpage)){
  subpage.1= subpage[j]
  r<-GET(subpage.1,user_agent("myua"))
   if(status_code(r)>300){ 
     company[j]=NA
     location[j]=NA 
     Python[j]=NA 
     R[j]=NA 
     SAS[j]=NA 
     SQL[j]=NA 
     Java[j]=NA 
     Tableau[j]=NA 
     Spark[j]=NA 
     C[j]=NA 
     Perl[j]=NA 
     Excel[j]=NA
   }
   else{
  job <-subpage.1 %>%html_session()
   tryCatch({job%>%read_html()%>%html_text()},error=function(e){
      company[j]=NA
     location[j]=NA })
  company[j] <- job %>%read_html() %>% html_node(".padRtSm") %>% html_text()
      location[j]<- job %>% html_node(".subtle") %>% html_text()

      tryCatch({readLines(subpage.1)},warning=function(e){
         industry[j]=NA},error=function(w){
  industry[j]=NA})
  desc.1 <- readLines(subpage.1,warn=FALSE)
    industry.1<- desc.1[str_detect(desc.1, "\'sector\'")] %>%
      str_extract("\"(.*)\"") %>% str_sub(2, -2)
    industry.1=unlist(industry.1,recursive = FALSE) 
  industry.1=ifelse(length(industry.1)==0,NA,industry.1) 
    industry[j]=industry.1

 tryCatch({job%>%read_html()%>%html_text()},error=function(f){
  c(Python[j],R[j],SAS[j],SQL[j],Java[j],Tableau[j],Spark[j],C[j],Perl[j],Excel[j],Hadoop[j],NoSQL[j], HBase[j])=rep(NA,13)})
desc <- job %>% read_html() %>% html_nodes("#JobDescContainer") %>% html_text()
  Python[j] <- any(grepl("python", desc, ignore.case=TRUE))
  R[j] <- any(grepl("\\bR\\b", desc, ignore.case=TRUE))
  SAS[j] <- any(grepl("\\bSAS\\b", desc, ignore.case=TRUE))
  SQL[j]<- any(grepl("\\bSQL\\b", desc, ignore.case=TRUE))
  Java[j] <- any(grepl("\\bJava\\b", desc, ignore.case=TRUE))
  Tableau[j] <- any(grepl("\\bTableau\\b", desc, ignore.case=TRUE))
  Spark[j] <- any(grepl("\\bSpark\\b", desc, ignore.case=TRUE))
   C[j]<- any(grepl("\\bC\\b", desc, ignore.case=TRUE))
   Perl[j] <- any(grepl("\\bPerl\\b", desc, ignore.case=TRUE))
   Excel[j] <- any(grepl("\\bExcel\\b", desc, ignore.case=TRUE))
   Hadoop[j]<- any(grepl("\\bHadoop\\b", desc, ignore.case=TRUE))
   NoSQL[j]<- any(grepl("\\bNoSQL\\b", desc, ignore.case=TRUE))
   HBase[j]<- any(grepl("\\bHBase\\b", desc, ignore.case=TRUE))
   Sys.sleep(1)
   }      
}
data <- data.frame("Company" = company, "Location"=location, "Industry"=industry,"Rating"=Rating,"Salaries"=Salaries,"Python"=Python, "R"=R,"SAS"=SAS,"SQL"=SQL,"Java"=Java,"Tableau"=Tableau,"Spark"=Spark,"C"=C,"Perl"=Perl,"Excel"=Excel,"Hadoop"=Hadoop,"NoSQL"=NoSQL,"HBase"=HBase)
data <-data[c(1:length(subpage)),]
data$Website<-subpage
data.1 <- data[,-19]
data.2 <- subset(data.1, !duplicated(data.1))
data.2$State<-str_sub(data.2$Location,-2,-1)
write.csv(data.2,file = "glass_door.csv")


#Collect job titled "data-engineer " 
url.statana<- paste0("https://www.glassdoor.com/Job/data-engineer-jobs-SRCH_KO0,13_IP", 1:20, ".htm")
subpage.statana<- rep(NA,500)
h=0
for (i in 1:length(url.statana)){
  url.2=url.statana[i]
   fields <- url.2 %>% read_html() %>% html_nodes(xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "jl", " " ))]')
    job.urls.1 <- fields%>% html_nodes("a.jobLink") %>% html_attr("href")
    job.urls.1 <-job.urls.1[c(seq(1,60,by=2))]
    job.urls<- unique(paste('https://www.glassdoor.com', job.urls.1, sep = ''))
    subpage.2=unlist(job.urls,recursive = FALSE)
    subpage.statana[(h+1):(h+length(subpage.2))]=subpage.2
   h=h+30
Sys.sleep(1)
}    

subpage.statana<- ifelse(grepl("NA",subpage.statana)==TRUE,NA,subpage.statana) 
subpage.statana<-subpage.statana[!is.na(subpage.statana)]

o=length(subpage.statana)
company.stat <- rep(NA, o)
location.stat <- rep(NA, o)
Python.stat<-rep(NA, o)
R.stat<-rep(NA, o)
SAS.stat<-rep(NA, o)
SQL.stat<-rep(NA, o)
Java.stat<-rep(NA,o)
Tableau.stat<-rep(NA, o)
Spark.stat<-rep(NA, o)
C.stat<-rep(NA, o)
Perl.stat<-rep(NA, o)
Excel.stat<-rep(NA, o)
Hadoop.stat<-rep(NA, o)
NoSQL.stat<-rep(NA, o)
HBase.stat<-rep(NA, o)


for(g in 1:length(subpage.statana)){
  subpage.1= subpage.statana[g]
  r<-GET(subpage.1,user_agent("myua"))
   if(status_code(r)>300){ 
     Python.stat[g]=NA 
     R.stat[g]=NA 
     SAS.stat[g]=NA 
     SQL.stat[g]=NA 
     Java.stat[g]=NA 
     Tableau.stat[g]=NA 
     Spark.stat[g]=NA 
     C.stat[g]=NA 
     Perl.stat[g]=NA 
     Excel.stat[g]=NA
   }
   else{job <-subpage.1 %>%html_session()
   tryCatch({job%>%read_html()%>%html_text()},error=function(e){
      company.stat[g]=NA
     location.stat[g]=NA })
  company.stat[g] <- job %>%read_html() %>% html_node(".padRtSm") %>% html_text()
      location.stat[g]<- job %>% html_node(".subtle") %>% html_text()

 tryCatch({job%>%read_html()%>%html_text()},error=function(g){
  c(Python.stat[g],R.stat[g],SAS.stat[g],SQL.stat[g],Java.stat[g],Tableau.stat[g],Spark.stat[g],C.stat[g],Perl.stat[g],Excel.stat[g],Hadoop.stat[g],NoSQL.stat[g], HBase.stat[g])=rep(NA,13)})
desc <- job %>% read_html() %>% html_nodes("#JobDescContainer") %>% html_text()
  Python.stat[g] <- any(grepl("python", desc, ignore.case=TRUE))
  R.stat[g] <- any(grepl("\\bR\\b", desc, ignore.case=TRUE))
  SAS.stat[g] <- any(grepl("\\bSAS\\b", desc, ignore.case=TRUE))
  SQL.stat[g]<- any(grepl("\\bSQL\\b", desc, ignore.case=TRUE))
  Java.stat[g] <- any(grepl("\\bJava\\b", desc, ignore.case=TRUE))
  Tableau.stat[g] <- any(grepl("\\bTableau\\b", desc, ignore.case=TRUE))
  Spark.stat[g] <- any(grepl("\\bSpark\\b", desc, ignore.case=TRUE))
   C.stat[g]<- any(grepl("\\bC\\b", desc, ignore.case=TRUE))
   Perl.stat[g] <- any(grepl("\\bPerl\\b", desc, ignore.case=TRUE))
   Excel.stat[g] <- any(grepl("\\bExcel\\b", desc, ignore.case=TRUE))
   Hadoop.stat[g]<- any(grepl("\\bHadoop\\b", desc, ignore.case=TRUE))
   NoSQL.stat[g]<- any(grepl("\\bNoSQL\\b", desc, ignore.case=TRUE))
   HBase.stat[g]<- any(grepl("\\bHBase\\b", desc, ignore.case=TRUE))
   Sys.sleep(1)
   }      
}

data.stat <- data.frame("Company" = company.stat, "Location"=location.stat, "Python"=Python.stat, "R"=R.stat,"SAS"=SAS.stat,"SQL"=SQL.stat,"Java"=Java.stat,"Tableau"=Tableau.stat,"Spark"=Spark.stat,"C"=C.stat,"Perl"=Perl.stat,"Excel"=Excel.stat,"Hadoop"=Hadoop.stat,"NoSQL"=NoSQL.stat,"HBase"=HBase.stat)
data.stat <-data.stat[c(1:length(subpage.statana)),]

data.stat.2 <- subset(data.stat, !duplicated(data.stat))
write.csv(data.stat.2,file = "glass_door_statana.csv")
```

#Step2: data cleaning
Remove all the redundant symbols. Count missing values and remove the missing rows. Seperate salaries into max and min and calculate the mean. Save the data set as "data.final.csv".
1.
```{r}
data.glassdoor<- read.csv("../../data/glass_door.csv")
Companyname_m<-length(which(is.na(data.glassdoor$Company)))
#102 positions miss companies' name

#remove "-" of location
data.glassdoor$Location<-gsub("–", "", as.character(data.glassdoor$Location))
#remove leading space of location
data.glassdoor$Location<-str_trim(data.glassdoor$Location,"left")
location_m.1<-length(which(is.na(data.glassdoor$Location)))
location_m.2<-length(which(data.glassdoor$Location=="Remote"))
location_m.3<-length(which(data.glassdoor$Location=="United States"))
#21 position miss location including "remote"and "United States"

indus_m<-length(which(is.na(data.glassdoor$Industry)))
#79 position miss Industry
data.glassdoor$Industry=gsub("&amp", "", as.character(data.glassdoor$Industry))

rating_m<-length(which(is.na(data.glassdoor$Rating)))
##41 position miss rating

skill_m<-length(which(is.na(data.glassdoor[,7:19])))
##13 position miss rating

#create a column named max salary
 data.glassdoor$max=rep(NA,nrow(data.glassdoor))
  data.glassdoor$min=rep(NA,nrow(data.glassdoor))

  for (f in 1:nrow(data.glassdoor)){
   s=data.glassdoor$Salaries[f]
    if (is.na(s) ){
    data.glassdoor$max[f]=NA
    data.glassdoor$min[f]=NA
  }
  else
   max1=unlist(strsplit(as.character(s),'-'))[2]
   max2=unlist(strsplit(as.character(max1),'k'))[1]
 data.glassdoor$max[f]=rm1stchar(max2, n = 1)[1]
     
  min1=unlist(strsplit(as.character(s),'-'))[1]
  min2=unlist(strsplit(as.character(min1),'k'))[1]
 data.glassdoor$min[f]=rm1stchar(min2, n = 1)[1]
  }
 
salary_m<-length(which(is.na(data.glassdoor[,21:22])))
##78 position miss salary 

 #remove missing values(industry,salary,rate,location)
data.final<-data.glassdoor[(complete.cases(data.glassdoor[3:22])),]   
data.final$mean<-(as.numeric(data.final$max)+as.numeric(data.final$min))/2
data.final<-data.final[-which(data.final$Location=="Remote"|data.final$Location=="United States"),]
data.final<-data.final[,-1]
table(data.final$Industry)
#Therefore, we have 639 positions totally
```

Save the data as data.final.csv
```{r,eval=FALSE}
write.csv(data.final,file = "data.final.csv")
```


#Step3:Exploratory analysis and Visualization
plot the top 10 skills for both data scientist and data engineer. 
Perform two-proportion test 
1.
```{r}
data.final<- read.csv("../../data/data.final.csv")
data.final<-data.final[,-1]
a=1
skill.whole<-rep(NA,13)
for(i in 6:18){
  skill.whole[a]<-length(which(data.final[,i]==TRUE))
a=a+1
  }
skills<-c("Python","R","SAS","SQL","Java","Tableau","Spark","C","Perl","Excel","Hadoop","NoSQL","HBase")
count.whole<-data.frame(as.numeric(unlist(skill.whole)))
count.whole$skills<-skills
colnames(count.whole)<-c("Count(DS","Skills")
count.whole$Count<-as.numeric(count.whole$Count)

#data engineer
skill_stat<-read.csv("../../data/glass_door_statana.csv")
skill_stat<-skill_stat[,-1]
b=1
skill_stat.1<-rep(NA,13)
for(i in 3:15){
  skill_stat.1[b]<-length(which(skill_stat[,i]==TRUE))
b=b+1
  }
count.whole$Count<-as.numeric(unlist(skill_stat.1))
count.whole<-count.whole[,c(2,1,3)]
colnames(count.whole)<-c("Skills","Count(DS)","Count(DE)")
count.whole$'Occurrence%(DS)'<-round((count.whole$`Count(DS)`)/nrow(data.final)*100,2)
count.whole$'Occurrence%(DE)'<-round((count.whole$`Count(DE)`)/nrow(skill_stat)*100,2)
count.whole<-arrange(count.whole,desc(`Count(DS)`))
count.whole.top10<-count.whole[1:10,]

count.whole.top10$Skills<-factor(count.whole.top10$Skills, levels = count.whole.top10$Skills[order(count.whole.top10$`Count(DS)`)])
count.whole.1<-melt(count.whole.top10[,c(1,4,5)],id.vars='Skills')
count.whole.1$positions<-c(rep("Data Scientist",10),rep("Data Engineer",10))


p_skills<-ggplot(data=count.whole.1,aes(Skills,value,fill=positions))+
  geom_bar(stat="identity",position="dodge",width = 0.8)+ 
  coord_flip() + labs(x = 'Skills', y = 'Occurrence(%)')+ggtitle('Occurrence(%) versus Skills for Data Scientist and Data Engineer')+theme(plot.title = element_text(size=12))+theme(axis.title.x = element_text(size =10))+theme(axis.text = element_text(siz=10 ))+theme(axis.title.y = element_text(size =10))+ 
    geom_text(aes(y = value + 9,    # nudge above top of bar
                  label = paste0(value, '%')),    # prettify
              position = position_dodge(width = .9), 
              size = 2)


#statistical analysis of R and SAS

test.R<-prop.test(c(count.whole.top10[2,2],count.whole[2,3]),c(nrow(data.final),nrow(skill_stat)),correct=FALSE,alternative="greater")

test.SAS<-prop.test(c(count.whole.top10[8,2],count.whole[8,3]),c(nrow(data.final),nrow(skill_stat)),correct=FALSE,alternative="greater")


skills.2<-c("R","SAS")
p.value<-c(test.R$p.value,test.SAS$p.value)
hyp.test<-as.data.frame(skills.2)
hyp.test$p<-p.value
hyp.test$`proportion difference`<-c(count.whole.top10[2,4]-count.whole.top10[2,5],count.whole.top10[8,4]-count.whole.top10[8,5])
colnames(hyp.test)<-c("Skill","p value","Proportion Difference(%)")

table.skill<-ggtexttable(hyp.test,theme = ttheme("mBlue"))

p.skill.t<-ggarrange(p_skills,table.skill,nrow= 2,labels = c('A', 'B'))

 
ggsave("../../figure/count_skills.png",p.skill.t,width = 7, height = 5)               
p.skill.t
```


Collect longitude and latitude of each location using function geocode. Data was saved as "data.location_1.csv". Code is not evaluated.
2.
```{r,eval=FALSE}
data.location<-as.data.frame(data.final$Location[!is.na(data.final$Location)==TRUE])
colnames(data.location)<-"Location"
data.location$lon<-rep(NA,nrow(data.location))
data.location$lat<-rep(NA,nrow(data.location))

for(l in 1:nrow(data.location)){
  ll=as.character(data.location$Location[l])
b=any(grepl("\\,", ll, ignore.case=TRUE))
      if(length(b)=="FALSE")
{data.location$lon=NA
data.location$lat=NA
}

else
data.location$lon[l]=geocode(ll)[1]
data.location$lat[l]=geocode(ll)[2]
Sys.sleep(1)
}

data.location_1<-as.data.frame(unlist(data.location$Location))
data.location_1$lon<-unlist(data.location$lon)
data.location_1$lat<-unlist(data.location$lat)
colnames(data.location_1)<-c("City","lon","lat")

write.csv(data.location_1,file ="data.location_1.csv")
```


plot US map using get_map
```{r}
map<-get_map(location='united states', zoom=4, maptype = "terrain",             source='google',color='color')
```

We geographically visualized the distribution of data scientist positions. Make a table of top five cities that hire most data scientists. We also ploted top 10 industries that hire most data scientists.
3.
```{r}
#Geographically visualized the distribution of data scientist positions
#make a table of top five cities that hire most data scientists
data.location_1= read.csv("../../data/data.location_1.csv")
data.location_1=data.location_1[,-1]
data.location_1<-data.location_1[(complete.cases(data.location_1)),]   
data.location_2=add_count(data.location_1, City)
data.location_2<-subset(data.location_2, !duplicated(data.location_2))
colnames(data.location_2)<-c("City","lon","lat","Count")

gmap=ggmap(map) +geom_point(aes(x=lon, y=lat,size=Count), data=data.location_2,alpha=0.5,col="orange")+ 
  xlab("longitude") +
  ylab("latitude")+scale_size_continuous(range = c(0.5, 7))


data.position.count<-data.location_2
data.position.count.2<-arrange(data.position.count,desc(Count))
data.position.count.3<-subset(data.position.count.2, !duplicated(data.position.count.2[,"City"]))

industryrank<-data.frame(table(data.final$Industry))
industryrank.1<- industryrank[order(industryrank$Freq,decreasing=TRUE),][1:5,]
colnames(industryrank.1)=c("Industry","Count")
industryrank.1$Industry<-  factor(industryrank.1$Industry, levels = industryrank.1$Industry[order(industryrank.1$Count)])
industryrank.1$Frequency<-round(((industryrank.1$Count)/nrow(data.final))*100,3)


p1<-ggplot(data=industryrank.1,aes(factor(Industry),y=Count,fill=Count),width = 2, height = 3,alpha=.8)+geom_bar(stat="identity",width=0.8)+coord_flip()+labs(y="Count",x="Industry",size=3)+ggtitle("Count versus Industry in the US")+theme(plot.title = element_text(size=12))+theme(axis.title.x = element_text(size =10))+theme(axis.text = element_text(siz=10 ))+theme(axis.title.y = element_text(size=10))+scale_fill_gradient2()
rownames(industryrank.1) <- c()


                          
ppp<-ggarrange(p1,gmap,nrow= 2,labels = c('A', 'B'))
ggsave("../../figure/indus.png",width = 7, height = 5)               
ppp

```


Step4:Statistical analysis 
1.
```{r,echo=FALSE,message=FALSE,warning=FALSE}
#Create a new data frame called data.model
#Classify the states into four regions:West(0), Midwest(1), South(2) and Northeast(3)
#Classify industries into 4 groups: 0.IT 1. Business Service 2.Finance 3.other industry
#Make a linear regression model to estimate how industry and region impact the average annual salaries
data.model<-data.final
data.model$State<-toupper(data.model$State)

data.model$region=rep(NA,nrow(data.model))
for(i in 1:nrow(data.model)){
  if(grepl("WA|OR|CA|MT|ID|WY|NV|AZ|UT|CO|NM", data.model$State[i])==TRUE){
    data.model$region[i]<-0
  }   
else if(grepl("ND|SD|NE|KS|MN|IA|MO|WI|IL|IN|MI|OH",data.model$State[i])==TRUE){
    data.model$region[i]<-1
  }   

  else if(grepl("TX|OK|AR|LA|MS|AL|TN|KY|GA|FL|SC|NV|VA|WV|DC|MD|DE|NC",data.model$State[i])==TRUE){
    data.model$region[i]<-2
  }   
  else if(grepl("PA|NJ|NY|CT|MA|VT|NH|ME|RI",data.model$State[i])==TRUE){
    data.model$region[i]<-3
  }   
  else
    data.model$region[i]<-NA
  }

data.model$ind<-rep(NA,nrow(data.model))
for (i in 1:nrow(data.model)){
  if(data.model$Industry[i]=="Information Technology"){
    data.model$ind[i]<-0
  }
 else if(data.model$Industry[i]=="Business Services"){
    data.model$ind[i]<-1
  }
   else if(data.model$Industry[i]=="Finance"){
    data.model$ind[i]<-2
   }
  else 
    data.model$ind[i]<-3
}
for (i in 6:18){
  data.model[,6:18]=ifelse(data.model[6:18]==TRUE,1,0)
}

#remove missing values(595 positions)
data.model<-data.model[complete.cases(data.model),]
#fit a linear regression model
fit.1<-lm(mean~factor(ind)+factor(region)+Rating+R+Python+SQL+Hadoop+Spark,data = data.model)
AICS<-stepAIC(fit.1,direction="both")
aicr<-AICS$anova
fit<-lm(mean~factor(ind)+factor(region)+Python+Spark,data = data.model)
summ<-summary(fit)
con<-confint(fit)
coef<-c("Intercept","Business Service","Finance","Other Industry","Midwest","South","Northeast","Python","Spark")
data.linear<-data.frame(summ$coefficients)
data.linear.1<-data.linear[,c(1,4)]
data.linear.2<-cbind(data.linear.1,con)
data.linear.3 <- data.frame(lapply(data.linear.2, function(y) if(is.numeric(y)) round(y, 3) else y)) 


colnames(data.linear.3)<-c("Estimate","p value","2.5 %","97.5 %")
rownames(data.linear.3)<-coef
data.linear.3<-data.linear.3[-c(2,4),]

ggtexttable(data.linear.3,theme = ttheme("mBlue"))
ggsave("../../figure/linear.final.png",width = 5, height = 3)  
```

appendix
1.
```{r}
#CSS selector and XPath
Information<-c("field","linking url","salary","rating","company's name","location","skills")
CSS<-c(".jl","a.jobLink","small",".compactStars",".padRtSm",".subtle","#JobDescContainer")
html.node<-data.frame(Information)
html.node$html_node<-CSS
ht<-ggtexttable(html.node,theme = ttheme("mBlue"))
ggsave("../../figure/html_node.png",ht,width = 5, height = 3)   
```

2.
```{r}
#skills and count
ggtexttable(count.whole,theme = ttheme("mBlue"))
ggsave("../../figure/count.whole.png",width = 7, height = 5)   
```

3.
```{r}
#industry count
ggtexttable(industryrank.1,theme = ttheme("mBlue"))
ggsave("../../figure/industryrank.1.png",width = 6, height = 5)   
```

4.
```{r}
#Top 5 cities that hire most data scientists
data.position.final<-ggtexttable(data.position.count.3[1:5,c(1,4)],theme = ttheme("mBlue"))
ggsave("../../figure/data.position.final.png",width = 5, height = 5)
data.position.final
```



```{r}
citation(package="rvest")
citation(package = "dplyr")
citation(package = "httr")

citation(package = "stringr")
citation(package = "curl")
citation(package = "hydroTSM")
citation(package = "tidyr")

citation(package = "ggplot2")
citation(package = "ggpubr")
citation(package = "gridExtra")
citation(package = "reshape2")
citation(package = "MASS")

citation(package = "ggmap")
citation(package = "grid")
citation(package = "knitr")


```

